{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import re \n",
    "import pandas as pd\n",
    "import PyPDF2 \n",
    "import random\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# Load the language model\n",
    "llm_ques = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-v0.1\",model_kwargs={\"temperature\": 0.05, \"max_new_tokens\": 1048})\n",
    "llm_ans = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-v0.1\",model_kwargs={\"temperature\": 0.3, \"max_new_tokens\": 1048})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_data(pdf_file_path):\n",
    "  \"\"\"\n",
    "  Extracts text and metadata from a PDF file.\n",
    "\n",
    "  Args:\n",
    "    pdf_file_path: The path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary containing the extracted data, including:\n",
    "      text: The complete text content of the PDF.\n",
    "      metadata: A dictionary of the PDF metadata (titles, author, etc.).\n",
    "  \"\"\"\n",
    "\n",
    "  # Open the PDF file\n",
    "  pdf_reader = PyPDF2.PdfReader(pdf_file_path)\n",
    "\n",
    "  # Extract text data\n",
    "  all_page_text = \"\"\n",
    "  for page in pdf_reader.pages:\n",
    "    page_text = page.extract_text()\n",
    "    all_page_text += page_text\n",
    "    \n",
    "    return all_page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(input_text):\n",
    "    # Remove noisy characters except for letters, digits, whitespace, ? and !\n",
    "    cleaned_text = re.sub(r'[^\\w\\s?!-, %]', '', input_text)\n",
    "\n",
    "    # Remove extra new lines\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(input_text):\n",
    "    # Remove noisy characters except for letters, digits, whitespace, ? and !\n",
    "    cleaned_text = re.sub(r'[^\\w\\s?!,\\-%\\(\\)]', '', input_text)\n",
    "\n",
    "    # Remove extra new lines\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def file_processing(file_path):\n",
    "    \"\"\"\n",
    "    This function processes a file to generate document-based questions and answers.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the document-based questions and answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data from PDF\n",
    "    loader = get_pdf_data(file_path)\n",
    "    page_content = loader\n",
    "\n",
    "    page_content = clean_up(page_content)\n",
    "        \n",
    "    splitter_ques_gen = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 100,\n",
    "        length_function = len,\n",
    "    )\n",
    "\n",
    "    chunks_ques = splitter_ques_gen.split_text(page_content)\n",
    "    print(\"Number of chunks: \", len(chunks_ques))\n",
    "\n",
    "    ques_gen = [Document(page_content=t) for t in chunks_ques]\n",
    "\n",
    "    splitter_ans_gen = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 300,\n",
    "        chunk_overlap = 100,\n",
    "        length_function = len,\n",
    "    )\n",
    "\n",
    "    answer_gen = splitter_ans_gen.split_documents(\n",
    "        ques_gen\n",
    "    )\n",
    "    print(\"Number of chunks: \", len(answer_gen))\n",
    "    return ques_gen, answer_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(file_path):\n",
    "    \"\"\"\n",
    "    This function represents the pipeline for generating questions and answers based on a given file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the answer generation chain and the filtered question list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the file to generate document-based questions and answers\n",
    "    print(\"Processing the file...\")\n",
    "    ques_gen, answer_gen = file_processing(file_path)\n",
    "\n",
    "    # Initialize the LLM question generation pipeline\n",
    "    ques_gen_pipeline = llm_ques\n",
    "\n",
    "    # Prompt template for generating questions\n",
    "    base_template = \"\"\"\n",
    "    As an expert in question generation, your task is to create insightful questions based on the given input.\n",
    "    Your goal is to help individuals gain a deeper understanding of the topic and encourage critical thinking.\n",
    "    Please generate questions based on the following input:\n",
    "\n",
    "    ------------\n",
    "\n",
    "    {text}\n",
    "\n",
    "    ------------\n",
    "\n",
    "    Your questions should explore different aspects of the input and test the knowledge base.\n",
    "    Ensure that no important information is overlooked and avoid repeating questions.\n",
    "    Feel free to ask factual and explanatory questions.\n",
    "\n",
    "    QUESTIONS:\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT_QUESTIONS = PromptTemplate(template=base_template, input_variables=[\"text\"])\n",
    "\n",
    "    # Template for refining existing questions\n",
    "    refined_template = (\"\"\"\n",
    "    As an expert in question generation, your task is to refine the existing questions based on the given context.\n",
    "    Your goal is to help individuals prepare for a knowledge test.\n",
    "    We have received some practice questions to a certain extent: {existing_answer}.\n",
    "    Now, we have additional context to consider:\n",
    "    ------------\n",
    "    {text}\n",
    "    ------------\n",
    "\n",
    "    Refine the original questions in English based on the new context.\n",
    "    If the context is not helpful, please provide the original questions.\n",
    "    QUESTIONS:\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "        input_variables=[\"existing_answer\", \"text\"],\n",
    "        template=refined_template,\n",
    "    )\n",
    "\n",
    "    # Load the LLM question generation chain\n",
    "    print(\"Loading the LLM question generation chain...\")\n",
    "    ques_gen_chain = load_summarize_chain(llm=ques_gen_pipeline, \n",
    "                                          chain_type=\"refine\", \n",
    "                                          verbose=True, \n",
    "                                          question_prompt=PROMPT_QUESTIONS, \n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)\n",
    "\n",
    "    # Generate questions using the LLM question generation chain\n",
    "    ques = ques_gen_chain.run(ques_gen)\n",
    "\n",
    "    # Split the generated questions into a list\n",
    "    ques_list = ques.split(\"\\n\")\n",
    "\n",
    "    # Filter the questions to include only those ending with '?' or '.'\n",
    "    filtered_ques_list = [element for element in ques_list if element.endswith('?') or element.endswith('.') and len(element) > 15]\n",
    "\n",
    "    filtered_ques_list =[string.strip() for string in filtered_ques_list]\n",
    "    \n",
    "    # Initialize the HuggingFace  embeddings\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    # Create a vector store using FAISS from the document-based answers\n",
    "    vector_store = FAISS.from_documents(answer_gen, embeddings)\n",
    "\n",
    "    # Initialize the LLM answer generation pipeline\n",
    "    answer_gen_pipeline = llm_ans\n",
    "\n",
    "\n",
    "    # Initialize the retrieval-based QA system using the LLM answer generation pipeline and the vector store\n",
    "    print(\"Initializing the retrieval-based QA system...\")\n",
    "    ans_gen_chain = RetrievalQA.from_chain_type(llm=answer_gen_pipeline, \n",
    "                                                          chain_type=\"stuff\",\n",
    "                                                          retriever=vector_store.as_retriever(k=2))\n",
    "\n",
    "    return ans_gen_chain, filtered_ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(file_path):\n",
    "    # Run the question-answering pipeline and get the answer generation chain and question list\n",
    "    print(\"Running the question-answering pipeline...\")\n",
    "    ans_gen_chain, ques_list = pipeline(file_path)\n",
    "    \n",
    "    # Create an empty dictionary to store the questions and answers\n",
    "    qa_dict = {\"Question\": [], \"Answer\": []}\n",
    "    selected_questions = random.sample(ques_list, 10)\n",
    "    \n",
    "    # Iterate through the question list\n",
    "\n",
    "    print(\"Generating answers...\")\n",
    "    for question in selected_questions:\n",
    "        print(\"Question: \", question)\n",
    "        \n",
    "        # Generate the answer using the answer generation chain\n",
    "        answer = ans_gen_chain.run(question)\n",
    "        print(\"Answer: \", answer)\n",
    "        print(\"--------------------------------------------------\\n\\n\")\n",
    "        \n",
    "        # Append the question and answer to the dictionary\n",
    "        qa_dict[\"Question\"].append(question)\n",
    "        qa_dict[\"Answer\"].append(answer)\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    qa_df = pd.DataFrame(qa_dict)\n",
    "    \n",
    "    return qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting...\")\n",
    "file_path = r\"D:\\Downloads\\Union_Budget_Analysis-2023-24.pdf\"\n",
    "gen(file_path)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
